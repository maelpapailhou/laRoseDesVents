C’est une forme de média plus récente, raison pour laquelle elle ne fait pas l’objet d’un encadrement aussi strict que celui de l’audiovisuel.

On a repris certaines dispositions de la **loi du 30 septembre 1986**, notamment avec la **loi du 21 juin 2004 pour la confiance dans l’économie numérique (LCEN)**. Cette loi définit, à l’article 1er, la communication en ligne au public comme :  
**« toute transmission sur demande individuelle de données numériques n’ayant pas le caractère de correspondance privée par un procédé de communication électronique permettant un échange réciproque d’informations entre l’émetteur et le récepteur »**.

Loin de prétendre régir l’ensemble des communications en ligne, la loi du 21 juin 2004 admet l’application du **droit commun**, notamment pour ce qui concerne la communication au public par voie électronique, qui obéit au **principe de liberté** et à la **responsabilité en cas d’abus**.

Pour certains, cette loi est venue réinstaurer un rééquilibrage, permettant à Internet de redonner du sens à la **liberté d’expression des idées et des opinions**. Les interventions du législateur restent **minimes**, se limitant à répondre à des besoins spécifiques. C’est le cas de la **loi du 25 février 2018**, qui est venue apporter des précisions relatives aux obligations des **opérateurs de services essentiels** ainsi qu’aux **fournisseurs de services numériques**.

Plus récemment, le **Règlement DSA** du **19 octobre 2022**, qui constitue l’un des grands chantiers de l’Union européenne, concerne **tous les intermédiaires en ligne** offrant leurs services sur le marché européen.

La communication en ligne présente toutefois de réels **défis**, du fait de l’évolution rapide des technologies. Le droit doit suivre ces évolutions, en définissant et en délimitant ces techniques ainsi que les pratiques afférentes, mais aussi en précisant les **différentes catégories d’opérateurs du numérique**, puisque chacun d’entre eux est associé à un régime particulier d’**obligations** et de **responsabilités**.
  
### Section 1 : La définition des médias et des technologies

Les **médias numériques** désignent une nouvelle génération de médias, née de l’application de l’informatique à la communication.

Ces médias combinent trois éléments :

- des **données** (texte, image, son),
- des **logiciels** (systèmes d’exploitation, navigateurs, etc.),
- du **matériel** (ordinateur, tablette, téléphone, etc.).

Les médias numériques regroupent tout type d’information transmise électroniquement, sous forme d’**audio, vidéo, image ou texte**. On peut les classer en fonction de leur **format**, mais aussi en fonction de la **nature du contenu** et de son **objectif**.

On retrouve notamment :

- les **sites web**, qui peuvent être informatifs,
- les **contenus de blogs**, un autre type de média numérique qui permet aux utilisateurs de communiquer avec leur public,
- les **publicités**, qui constituent un type important de média numérique,
- les **médias sociaux**,
- les **jeux vidéo** et les **plateformes de streaming**.

Enfin, la question de l’**IPTV** brouille la frontière entre **médias audiovisuels traditionnels** et **médias numériques**.

### Section 2 : Les opérateurs du numérique et leurs obligations

Les **acteurs du numérique** sont nombreux et chacun joue un rôle spécifique dans l’**écosystème numérique**. Ils participent à la **production**, à la **diffusion**, à la **gestion** et à la **régulation** des services et technologies numériques.

Il faut dès lors distinguer **deux grandes catégories d’opérateurs numériques**, ce qui permet de différencier les règles applicables :

- celles relatives aux **éditeurs de services en ligne**,
- celles applicables aux **fournisseurs de services intermédiaires**.
  
#### A) Les éditeurs de services en ligne

##### 1) La notion d’éditeur de services en ligne

La loi du **21 juin 2004 (LCEN)** ne définit pas à proprement parler les **éditeurs de services en ligne**. Toutefois, elle définit le **service édité**.  
À partir de cette définition, on considère qu’un **éditeur** est une personne physique ou morale qui exerce un **contrôle actif** sur le choix des contenus mis à disposition du public.

Il publie ou diffuse des contenus sur **Internet**, une **application** ou toute autre **plateforme numérique**. Cela peut inclure :

- les **entreprises**,
- les **médias**,
- les **blogueurs**,
- ou toute personne qui publie des informations ou des services en ligne.

L’éditeur se distingue de l’**hébergeur**, qui se limite à une fonction **technique et passive** de stockage, sans rôle de sélection.
##### 2) Les obligations applicables aux éditeurs de services en ligne

Le principe est celui de la **liberté de communication en ligne**. C’est une liberté **encadrée** par des obligations issues du droit interne et européen, renforcées par la **loi du 24 août 2021** (confortant le respect des principes de la République) et par le **règlement DSA**, entré en vigueur le **17 février 2024**.

Il existe d’importantes **limites** à cette liberté : les éditeurs doivent informer les utilisateurs de manière **claire et accessible** sur un certain nombre de points.

- **Article 6 LCEN** → devoir de transparence : indiquer l’identité de l’éditeur (nom, dénomination ou raison sociale, adresse postale et électronique, coordonnées téléphoniques et tout élément d’identification de l’entreprise, le cas échéant).
- **Conditions générales d’utilisation** → elles doivent être mises à disposition des utilisateurs.
- **Informations sur les prix et modalités de paiement**, le cas échéant.
- **Transparence algorithmique et modération** → obligation de donner une information claire sur les outils utilisés.
- **Obligation de signalement et de retrait** → depuis la loi du 24 août 2021 et le DSA, les éditeurs doivent mettre en place des mécanismes accessibles permettant de signaler les contenus illicites et doivent ensuite traiter ces signalements.
- **Rapports de transparence** → certaines plateformes doivent publier régulièrement des rapports sur leurs pratiques de modération, les signalements reçus et les mesures prises.

Ainsi, il existe un équilibre à trouver entre **liberté d’expression** et **liberté numérique**, avec des **obligations destinées à garantir un espace en ligne sûr**.

##### 3) Le régime de responsabilité des éditeurs. 

Les **éditeurs de services en ligne** sont considérés comme pleinement responsables de l’ensemble des contenus figurant sur leur site Internet, **contrairement aux hébergeurs**.  
Ils sont responsables des contenus qu’ils **produisent** ou **diffusent**, ce qui peut inclure des entreprises, des plateformes de streaming ou encore des médias.

L’éditeur de service peut donc voir sa **responsabilité engagée** :

- **pour ses propres contenus**,
- mais aussi **pour ceux publiés par les utilisateurs**, dès lors que son rôle de modération dépasse une simple surveillance technique.

En effet, lorsqu’il dispose de **pleins pouvoirs de modération**, à la fois **a priori** (avant publication) et **a posteriori** (après publication), il a une véritable **obligation de vigilance**.

- **Responsabilité directe** : l’éditeur est responsable de ses propres contenus.
- **Responsabilité indirecte** : l’éditeur peut être tenu responsable des contenus publiés par les utilisateurs, si son rôle de modération n’est pas limité à une surveillance purement technique.

Une distinction s’opère entre :
- la **modération a priori** → contrôle exercé avant publication,
- la **modération a posteriori** → l’éditeur laisse publier les contenus puis les contrôle après.

Dans ce dernier cas, la **responsabilité de l’éditeur dépend de sa réactivité** face aux contenus illicites.

### B) Les fournisseurs de services intermédiaires : les hébergeurs et les plateformes en ligne

#### 1) La notion de fournisseur de service intermédiaire

La **loi LCEN de 2004** ne donne pas de définition précise, mais elle indique ce qu’elle recouvre. Cette loi distingue les acteurs techniques de l’internet : les **hébergeurs**, les **fournisseurs d’accès** et les **opérateurs de réseau**.

À la différence des éditeurs de services en ligne, ces acteurs ne produisent pas eux-mêmes les contenus, mais fournissent les moyens techniques permettant leur diffusion ou leur stockage.

Le **DSA** est venu préciser cette classification à l’échelle européenne. Il distingue trois catégories :
- les **services de simple transport**, qui transmettent les données sans en modifier le contenu (exemple : les fournisseurs d’accès à internet) ;
- les **services de mise en cache**, qui stockent temporairement les données pour en faciliter l’accès ;
- les **services d’hébergement**, qui stockent durablement les données à la demande d’un utilisateur (exemple : plateformes de partage, forums, réseaux sociaux).

Ces différents intermédiaires sont soumis à un **régime de responsabilité limitée**, car on considère qu’ils ne sont pas responsables des contenus publiés par les utilisateurs, sauf s’ils avaient connaissance de leur caractère illicite et n’ont pas agi promptement pour les retirer. Ce principe, issu de la LCEN, a été confirmé par le DSA.

On distingue donc la **responsabilité pleine et entière des éditeurs de services en ligne** et la **responsabilité limitée des fournisseurs de services intermédiaires**.

#### 2) Les obligations applicalbes aux fournisseurs de services intermédiaires

Si la responsabilité des intermédiaires est limitée, elle est assortie de **devoirs de diligence** renforcés ces dernières années.

Les plateformes et les hébergeurs doivent prévoir des **mécanismes de signalement accessibles**, prévus notamment par l’article 16 du DSA. Ces mécanismes doivent permettre d’alerter en cas de publication illégale sur internet ou sur une plateforme en ligne. Ils doivent être **visibles et accessibles à tous**.

Il existe également une **obligation de retrait rapide des contenus illicites**. Les hébergeurs doivent retirer ou rendre inaccessibles les contenus manifestement illicites, par exemple les contenus d’incitation à la haine, liés au terrorisme ou à la pédopornographie.

À la suite de ces signalements, les hébergeurs doivent **prévenir les autorités compétentes**, notamment le procureur de la République.

On peut noter que certains hébergeurs ont mis en place leurs propres mécanismes de signalement, avec des conditions et caractéristiques propres à chaque plateforme (réseaux sociaux tels que X, Instagram, Snapchat, TikTok, etc.).

Si certaines plateformes respectent cette obligation de signalement, d’autres semblent bâtir leur succès sur l’absence de régulation. C’est le cas de certaines plateformes de jeux d’argent ou de diffusion de contenus « trash », comme la plateforme Kick. Toutefois, ces plateformes ne sont pas exemptées des obligations du DSA, qui s’appliquent à **toute plateforme ciblant des utilisateurs situés dans l’Union européenne**, quel que soit le pays d’implantation de la plateforme.

Au-delà de ces obligations, il existe un **devoir général de coopération avec les autorités judiciaires et administratives**, prévu à la fois par la LCEN et par la loi du 24 août 2021.

Les intermédiaires doivent notamment **conserver et transmettre certaines données d’identification** en cas d’enquête pénale.

Ils sont également soumis à une **obligation de transparence** : les plateformes doivent publier des rapports sur la modération des contenus, les notifications reçues et les mesures prises.

Le DSA prévoit des **obligations renforcées pour les grandes plateformes en ligne**. Il classe les plateformes ou moteurs de recherche ayant plus de **45 millions d’utilisateurs par mois dans l’UE** comme **VLOP** (Very Large Online Platforms) ou **VLOSE** (Very Large Online Search Engines).

Ces acteurs sont soumis à des obligations spécifiques :
- audit annuel indépendant ;
- évaluation et limitation des risques systémiques ;
- accès renforcé aux données pour les chercheurs agréés ;
- supervision par la **Commission européenne**, et non uniquement par les régulateurs nationaux.

On observe donc une évolution vers un **modèle de régulation européenne renforcée**. Alors que la LCEN organisait surtout un régime de responsabilité limitée au niveau national, le DSA établit un **cadre harmonisé européen** avec un contrôle accru des grandes plateformes.

Enfin, le **projet « ChatControl »**, présenté par le Parlement européen et le Conseil en mai 2022, prévoit d’obliger les fournisseurs de services de communication à **scanner les messages privés, même chiffrés**, afin de lutter contre les contenus pédopornographiques. Ce projet, qui impose aux plateformes de scanner tous les messages privés, a suscité de vives discussions, au point qu’une pétition a été créée pour le contrer.

S’il poursuit l’objectif d’un **cadre européen unifié** destiné à remplacer les régulations nationales disparates, il soulève de nombreux risques en matière de **droit à la vie privée** et de **protection des données personnelles**, puisqu’il concernerait l’ensemble des communications privées.

### C) L'intelligence artificielle dans les médias

C’est un aspect important qui a émergé ces dernières années. L’**intelligence artificielle (IA)** prend une place grandissante dans le domaine des médias, transformant à la fois la manière dont les contenus sont produits et diffusés, mais aussi les responsabilités des opérateurs numériques liés à ces technologies.

Plusieurs enjeux font l’objet d’une attention particulière : d’abord la **qualité de l’information et la lutte contre la désinformation**, ensuite les **défis liés à la transparence algorithmique**, et enfin la **question de la propriété intellectuelle et de la création de contenu par l’IA**.

#### 1) L’impact de l’IA sur la qualité de l’information

Il faut envisager cette question dans les deux sens. À première vue, les outils d’IA (générateurs de texte, de synthèse vocale ou visuelle) peuvent être considérés comme des atouts puisqu’ils facilitent la production rapide de contenu médiatique. Cependant, ils peuvent aussi être utilisés de manière inverse, notamment pour créer des **fake news**, c’est-à-dire des informations trompeuses ou erronées, parfois intentionnellement.

Le développement des **deepfakes (hypertrucages)** illustre ce risque. Il s’agit de vidéos ou d’images falsifiées grâce à l’IA, permettant de manipuler les apparences ou les discours. Ces procédés constituent une menace particulière pour l’intégrité de l’information.

À l’origine, le deepfake n’est pas illégal : c’est une **technique licite**, utilisée dans le cadre de la liberté d’expression pour générer du contenu audio ou vidéo, et exploitée depuis longtemps dans le cinéma ou la création artistique. Mais dans la pratique, il est souvent **détourné à des fins malveillantes**, afin de tromper les personnes qui visionnent un contenu truqué en pensant que la personne représentée en est réellement l’auteur.

C’est pourquoi on tend à vouloir **encadrer** cette pratique, notamment par une **obligation de transparence**. À ce sujet, la **loi SREN du 21 mai 2024** (visant à sécuriser et réguler l’espace numérique) prévoit des sanctions : **1 an d’emprisonnement et 15 000 € d’amende** pour avoir diffusé au public ou à un tiers un contenu visuel ou sonore généré par un traitement algorithmique représentant une personne, sans autorisation et sans mention explicite de son caractère artificiel. Ces sanctions peuvent aller jusqu’à **2 ans d’emprisonnement et 45 000 € d’amende** lorsque la diffusion se fait via un service en ligne (article 15 de la loi SREN, modifiant l’article 226 du Code pénal).

Ainsi, l’utilisation reste possible, mais elle doit être **explicitement signalée**.

Concernant les **fake news**, la **loi du 22 décembre 2018 n° 2018-1202 relative à la lutte contre la manipulation de l’information** impose aux plateformes un **devoir de coopération**. Elles doivent mettre en place des mesures pour limiter la diffusion de fausses informations susceptibles de troubler l’ordre public. Ces mesures incluent :

- un dispositif de signalement **facilement accessible et visible** ;
- une exigence de **transparence des algorithmes** des plateformes.

Cette loi a aussi instauré une **responsabilité renforcée** : dans le cadre électoral, si des informations fausses ou trompeuses susceptibles de fausser le scrutin sont diffusées massivement et délibérément, le juge peut être saisi (par le ministère public, un candidat ou un parti) et doit statuer sous **48 heures**. En cas d’appel, la cour doit également statuer sous le même délai.

#### 2) La transparence des algorithmes

Les systèmes d’IA sont largement mobilisés dans les médias, notamment pour **personnaliser les recommandations de contenu** en fonction des préférences des utilisateurs.

Cependant, ces algorithmes peuvent avoir un **effet pervers** : ils créent ou renforcent le phénomène des **bulles informationnelles**, qui enferment les internautes dans des points de vue partiels ou biaisés. En établissant un profil basé sur le comportement de l’utilisateur, les algorithmes proposent en priorité des contenus conformes à ses opinions, ce qui réduit la diversité des informations accessibles.

Pour répondre à ces défis, les opérateurs numériques doivent garantir une **transparence accrue**. Le **DSA** et le **RGPD** imposent en effet des obligations de transparence sur les critères utilisés pour recommander ou filtrer les contenus (**article 27 du DSA**).

Ces informations doivent apparaître dans les **conditions générales d’utilisation** ou la **politique de confidentialité**.

De plus, selon l’**article 38 du DSA**, les très grandes plateformes doivent proposer **au moins une option de recommandation non fondée sur le profilage**.

À venir, l’**AI Act** prévoit également des obligations de transparence, par exemple l’**étiquetage obligatoire des deepfakes** et l’**information claire lorsque l’on interagit avec un système d’IA**.